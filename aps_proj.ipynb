{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = pd.read_csv('ida_2016_training_set_update.csv', sep=',', header=0, na_values=\"na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes =  171\n",
      "Number of data =  60000\n",
      "*******************\n",
      "----- Column Headers -----\n",
      "['class' 'aa_000' 'ab_000' 'ac_000' 'ad_000' 'ae_000' 'af_000' 'ag_000'\n",
      " 'ag_001' 'ag_002' 'ag_003' 'ag_004' 'ag_005' 'ag_006' 'ag_007' 'ag_008'\n",
      " 'ag_009' 'ah_000' 'ai_000' 'aj_000' 'ak_000' 'al_000' 'am_0' 'an_000'\n",
      " 'ao_000' 'ap_000' 'aq_000' 'ar_000' 'as_000' 'at_000' 'au_000' 'av_000'\n",
      " 'ax_000' 'ay_000' 'ay_001' 'ay_002' 'ay_003' 'ay_004' 'ay_005' 'ay_006'\n",
      " 'ay_007' 'ay_008' 'ay_009' 'az_000' 'az_001' 'az_002' 'az_003' 'az_004'\n",
      " 'az_005' 'az_006' 'az_007' 'az_008' 'az_009' 'ba_000' 'ba_001' 'ba_002'\n",
      " 'ba_003' 'ba_004' 'ba_005' 'ba_006' 'ba_007' 'ba_008' 'ba_009' 'bb_000'\n",
      " 'bc_000' 'bd_000' 'be_000' 'bf_000' 'bg_000' 'bh_000' 'bi_000' 'bj_000'\n",
      " 'bk_000' 'bl_000' 'bm_000' 'bn_000' 'bo_000' 'bp_000' 'bq_000' 'br_000'\n",
      " 'bs_000' 'bt_000' 'bu_000' 'bv_000' 'bx_000' 'by_000' 'bz_000' 'ca_000'\n",
      " 'cb_000' 'cc_000' 'cd_000' 'ce_000' 'cf_000' 'cg_000' 'ch_000' 'ci_000'\n",
      " 'cj_000' 'ck_000' 'cl_000' 'cm_000' 'cn_000' 'cn_001' 'cn_002' 'cn_003'\n",
      " 'cn_004' 'cn_005' 'cn_006' 'cn_007' 'cn_008' 'cn_009' 'co_000' 'cp_000'\n",
      " 'cq_000' 'cr_000' 'cs_000' 'cs_001' 'cs_002' 'cs_003' 'cs_004' 'cs_005'\n",
      " 'cs_006' 'cs_007' 'cs_008' 'cs_009' 'ct_000' 'cu_000' 'cv_000' 'cx_000'\n",
      " 'cy_000' 'cz_000' 'da_000' 'db_000' 'dc_000' 'dd_000' 'de_000' 'df_000'\n",
      " 'dg_000' 'dh_000' 'di_000' 'dj_000' 'dk_000' 'dl_000' 'dm_000' 'dn_000'\n",
      " 'do_000' 'dp_000' 'dq_000' 'dr_000' 'ds_000' 'dt_000' 'du_000' 'dv_000'\n",
      " 'dx_000' 'dy_000' 'dz_000' 'ea_000' 'eb_000' 'ec_00' 'ed_000' 'ee_000'\n",
      " 'ee_001' 'ee_002' 'ee_003' 'ee_004' 'ee_005' 'ee_006' 'ee_007' 'ee_008'\n",
      " 'ee_009' 'ef_000' 'eg_000']\n",
      "----- Display top rows -----\n",
      "  class  aa_000  ab_000        ac_000  ad_000  ae_000  af_000  ag_000  ag_001  \\\n",
      "0   neg   76698     NaN  2.130706e+09   280.0     0.0     0.0     0.0     0.0   \n",
      "1   neg   33058     NaN  0.000000e+00     NaN     0.0     0.0     0.0     0.0   \n",
      "2   neg   41040     NaN  2.280000e+02   100.0     0.0     0.0     0.0     0.0   \n",
      "3   neg      12     0.0  7.000000e+01    66.0     0.0    10.0     0.0     0.0   \n",
      "4   neg   60874     NaN  1.368000e+03   458.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   ag_002  ...     ee_002    ee_003    ee_004    ee_005    ee_006    ee_007  \\\n",
      "0     0.0  ...  1240520.0  493384.0  721044.0  469792.0  339156.0  157956.0   \n",
      "1     0.0  ...   421400.0  178064.0  293306.0  245416.0  133654.0   81140.0   \n",
      "2     0.0  ...   277378.0  159812.0  423992.0  409564.0  320746.0  158022.0   \n",
      "3     0.0  ...      240.0      46.0      58.0      44.0      10.0       0.0   \n",
      "4     0.0  ...   622012.0  229790.0  405298.0  347188.0  286954.0  311560.0   \n",
      "\n",
      "     ee_008  ee_009  ef_000  eg_000  \n",
      "0   73224.0     0.0     0.0     0.0  \n",
      "1   97576.0  1500.0     0.0     0.0  \n",
      "2   95128.0   514.0     0.0     0.0  \n",
      "3       0.0     0.0     4.0    32.0  \n",
      "4  433954.0  1218.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 171 columns]\n",
      "----- Data-Set Statistics -----\n",
      "        class        aa_000        ab_000        ac_000        ad_000  \\\n",
      "count   60000  6.000000e+04  13671.000000  5.666500e+04  4.513900e+04   \n",
      "unique      2           NaN           NaN           NaN           NaN   \n",
      "top       neg           NaN           NaN           NaN           NaN   \n",
      "freq    59000           NaN           NaN           NaN           NaN   \n",
      "mean      NaN  5.933650e+04      0.713189  3.560143e+08  1.906206e+05   \n",
      "std       NaN  1.454301e+05      3.478962  7.948749e+08  4.040441e+07   \n",
      "min       NaN  0.000000e+00      0.000000  0.000000e+00  0.000000e+00   \n",
      "25%       NaN  8.340000e+02      0.000000  1.600000e+01  2.400000e+01   \n",
      "50%       NaN  3.077600e+04      0.000000  1.520000e+02  1.260000e+02   \n",
      "75%       NaN  4.866800e+04      0.000000  9.640000e+02  4.300000e+02   \n",
      "max       NaN  2.746564e+06    204.000000  2.130707e+09  8.584298e+09   \n",
      "\n",
      "              ae_000        af_000        ag_000        ag_001        ag_002  \\\n",
      "count   57500.000000  57500.000000  5.932900e+04  5.932900e+04  5.932900e+04   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean        6.819130     11.006817  2.216364e+02  9.757223e+02  8.606015e+03   \n",
      "std       161.543373    209.792592  2.047846e+04  3.420053e+04  1.503220e+05   \n",
      "min         0.000000      0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%         0.000000      0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%         0.000000      0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%         0.000000      0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max     21050.000000  20070.000000  3.376892e+06  4.109372e+06  1.055286e+07   \n",
      "\n",
      "        ...        ee_002        ee_003        ee_004        ee_005  \\\n",
      "count   ...  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
      "unique  ...           NaN           NaN           NaN           NaN   \n",
      "top     ...           NaN           NaN           NaN           NaN   \n",
      "freq    ...           NaN           NaN           NaN           NaN   \n",
      "mean    ...  4.454897e+05  2.111264e+05  4.457343e+05  3.939462e+05   \n",
      "std     ...  1.155540e+06  5.433188e+05  1.168314e+06  1.121044e+06   \n",
      "min     ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%     ...  2.936000e+03  1.166000e+03  2.700000e+03  3.584000e+03   \n",
      "50%     ...  2.337960e+05  1.120860e+05  2.215180e+05  1.899880e+05   \n",
      "75%     ...  4.383960e+05  2.182320e+05  4.666140e+05  4.032220e+05   \n",
      "max     ...  7.793393e+07  3.775839e+07  9.715238e+07  5.743524e+07   \n",
      "\n",
      "              ee_006        ee_007        ee_008        ee_009        ef_000  \\\n",
      "count   5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  57276.000000   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean    3.330582e+05  3.462714e+05  1.387300e+05  8.388915e+03      0.090579   \n",
      "std     1.069160e+06  1.728056e+06  4.495100e+05  4.747043e+04      4.368855   \n",
      "min     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00      0.000000   \n",
      "25%     5.120000e+02  1.100000e+02  0.000000e+00  0.000000e+00      0.000000   \n",
      "50%     9.243200e+04  4.109800e+04  3.812000e+03  0.000000e+00      0.000000   \n",
      "75%     2.750940e+05  1.678140e+05  1.397240e+05  2.028000e+03      0.000000   \n",
      "max     3.160781e+07  1.195801e+08  1.926740e+07  3.810078e+06    482.000000   \n",
      "\n",
      "              eg_000  \n",
      "count   57277.000000  \n",
      "unique           NaN  \n",
      "top              NaN  \n",
      "freq             NaN  \n",
      "mean        0.212756  \n",
      "std         8.830641  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max      1146.000000  \n",
      "\n",
      "[11 rows x 171 columns]\n",
      "Number of positive classes =  1000\n",
      "Number of negative classes =  59000\n",
      "*******************\n",
      "Index(['br_000', 'bq_000', 'bp_000', 'bo_000', 'cr_000', 'ab_000', 'bn_000',\n",
      "       'bm_000'],\n",
      "      dtype='object')\n",
      "Training data-set shape after dropping features is  (60000, 163)\n",
      "              class        aa_000        ac_000        ad_000        ae_000  \\\n",
      "count  60000.000000  6.000000e+04  5.666500e+04  4.513900e+04  57500.000000   \n",
      "mean       0.016667  5.933650e+04  3.560143e+08  1.906206e+05      6.819130   \n",
      "std        0.128020  1.454301e+05  7.948749e+08  4.040441e+07    161.543373   \n",
      "min        0.000000  0.000000e+00  0.000000e+00  0.000000e+00      0.000000   \n",
      "25%        0.000000  8.340000e+02  1.600000e+01  2.400000e+01      0.000000   \n",
      "50%        0.000000  3.077600e+04  1.520000e+02  1.260000e+02      0.000000   \n",
      "75%        0.000000  4.866800e+04  9.640000e+02  4.300000e+02      0.000000   \n",
      "max        1.000000  2.746564e+06  2.130707e+09  8.584298e+09  21050.000000   \n",
      "\n",
      "             af_000        ag_000        ag_001        ag_002        ag_003  \\\n",
      "count  57500.000000  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
      "mean      11.006817  2.216364e+02  9.757223e+02  8.606015e+03  8.859128e+04   \n",
      "std      209.792592  2.047846e+04  3.420053e+04  1.503220e+05  7.617312e+05   \n",
      "min        0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%        0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%        0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%        0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    20070.000000  3.376892e+06  4.109372e+06  1.055286e+07  6.340207e+07   \n",
      "\n",
      "       ...        ee_002        ee_003        ee_004        ee_005  \\\n",
      "count  ...  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04   \n",
      "mean   ...  4.454897e+05  2.111264e+05  4.457343e+05  3.939462e+05   \n",
      "std    ...  1.155540e+06  5.433188e+05  1.168314e+06  1.121044e+06   \n",
      "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    ...  2.936000e+03  1.166000e+03  2.700000e+03  3.584000e+03   \n",
      "50%    ...  2.337960e+05  1.120860e+05  2.215180e+05  1.899880e+05   \n",
      "75%    ...  4.383960e+05  2.182320e+05  4.666140e+05  4.032220e+05   \n",
      "max    ...  7.793393e+07  3.775839e+07  9.715238e+07  5.743524e+07   \n",
      "\n",
      "             ee_006        ee_007        ee_008        ee_009        ef_000  \\\n",
      "count  5.932900e+04  5.932900e+04  5.932900e+04  5.932900e+04  57276.000000   \n",
      "mean   3.330582e+05  3.462714e+05  1.387300e+05  8.388915e+03      0.090579   \n",
      "std    1.069160e+06  1.728056e+06  4.495100e+05  4.747043e+04      4.368855   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00      0.000000   \n",
      "25%    5.120000e+02  1.100000e+02  0.000000e+00  0.000000e+00      0.000000   \n",
      "50%    9.243200e+04  4.109800e+04  3.812000e+03  0.000000e+00      0.000000   \n",
      "75%    2.750940e+05  1.678140e+05  1.397240e+05  2.028000e+03      0.000000   \n",
      "max    3.160781e+07  1.195801e+08  1.926740e+07  3.810078e+06    482.000000   \n",
      "\n",
      "             eg_000  \n",
      "count  57277.000000  \n",
      "mean       0.212756  \n",
      "std        8.830641  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        0.000000  \n",
      "75%        0.000000  \n",
      "max     1146.000000  \n",
      "\n",
      "[8 rows x 163 columns]\n"
     ]
    }
   ],
   "source": [
    "total_num_data = len(df_train_data.index)\n",
    "print(\"Number of attributes = \", len(df_train_data.columns))\n",
    "print(\"Number of data = \", total_num_data)\n",
    "print(\"*******************\")\n",
    "\n",
    "# Display all column headers\n",
    "print(\"----- Column Headers -----\")\n",
    "print(df_train_data.columns.values)\n",
    "\n",
    "# Display the first n rows\n",
    "print(\"----- Display top rows -----\")\n",
    "print(df_train_data.head(n=5))\n",
    "#\n",
    "# Describe the statistics of the data-set\n",
    "print(\"----- Data-Set Statistics -----\")\n",
    "print(df_train_data.describe(include=\"all\"))\n",
    "\n",
    "# Print number of positive classes and number of negative classes in the training data-set\n",
    "print(\"Number of positive classes = \", sum(df_train_data['class'] == 'pos'))\n",
    "print(\"Number of negative classes = \", sum(df_train_data['class'] == 'neg'))\n",
    "print(\"*******************\")\n",
    "\n",
    "# Replace class labels with integer values (neg = 0, pos = 1) in training and test data-set\n",
    "df_train_data['class'].replace({\n",
    "    'neg': 0,\n",
    "    'pos': 1\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Compute the percentage of missing data for each attribute in the training data set\n",
    "missing_percent_threshold = 0.70\n",
    "missing_data_count = pd.DataFrame(df_train_data.isnull().sum().sort_values(ascending=False), columns=['Number'])\n",
    "missing_data_percent = pd.DataFrame(df_train_data.isnull().sum().sort_values(ascending=False)/total_num_data, columns=['Percent'])\n",
    "missing_data = pd.concat([missing_data_count, missing_data_percent], axis=1)\n",
    "# print(missing_data)\n",
    "missing_column_headers = missing_data[missing_data['Percent'] > missing_percent_threshold].index\n",
    "print(missing_column_headers)\n",
    "\n",
    "# Drop the features with high amount of missing data in both train and test data-set\n",
    "df_train_data = df_train_data.drop(columns=missing_column_headers)\n",
    "print(\"Training data-set shape after dropping features is \", df_train_data.shape)\n",
    "print(df_train_data.describe())\n",
    "\n",
    "# Extract features and labels from the training and test data-set\n",
    "y_train = df_train_data.loc[:, 'class']\n",
    "x_train = df_train_data.drop('class', axis=1)\n",
    "\n",
    "corrmat = x_train.corr()\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);\n",
    "plt.show()\n",
    "\n",
    "# Fill missing data in training and test data-set\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "imputer_median.fit(x_train.values)\n",
    "x_train = imputer_median.transform(x_train.values)\n",
    "\n",
    "\n",
    "# Standardize the training and test data-set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "\n",
    "#  Synthetic Minority Oversampling Technique to balance the training data-set\n",
    "sm = SMOTE()\n",
    "x_train, y_train = sm.fit_sample(x_train, y_train)\n",
    "\n",
    "# Principal Component Analysis\n",
    "pca = PCA(n_components=0.75)\n",
    "pca.fit(x_train)\n",
    "x_train = pca.transform(x_train)\n",
    "\n",
    "\n",
    "corrmat_pca = pd.DataFrame(x_train).corr()\n",
    "sns.heatmap(corrmat_pca, vmax=.8, square=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18928   672]\n",
      " [ 1571 17769]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94     19600\n",
      "           1       0.96      0.92      0.94     19340\n",
      "\n",
      "    accuracy                           0.94     38940\n",
      "   macro avg       0.94      0.94      0.94     38940\n",
      "weighted avg       0.94      0.94      0.94     38940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(Y_test,y_pred))\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18302  1298]\n",
      " [  924 18416]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     19600\n",
      "           1       0.93      0.95      0.94     19340\n",
      "\n",
      "    accuracy                           0.94     38940\n",
      "   macro avg       0.94      0.94      0.94     38940\n",
      "weighted avg       0.94      0.94      0.94     38940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='poly', degree=3)\n",
    "svclassifier.fit(X_train, Y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(Y_test,y_pred))\n",
    "print(classification_report(Y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18841   759]\n",
      " [ 1184 18156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     19600\n",
      "           1       0.96      0.94      0.95     19340\n",
      "\n",
      "    accuracy                           0.95     38940\n",
      "   macro avg       0.95      0.95      0.95     38940\n",
      "weighted avg       0.95      0.95      0.95     38940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(X_train, Y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(Y_test,y_pred))\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9871597329224447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "# prediction on test set\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_disp = plot_roc_curve(svclassifier, X_test, Y_test)\n",
    "plt.show()\n",
    "ax = plt.gca()\n",
    "rfc_disp = plot_roc_curve(clf, X_test, Y_test, ax=ax, alpha=0.8)\n",
    "svc_disp.plot(ax=ax, alpha=0.8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
